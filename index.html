
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>aDiner</title>
    <meta name="author" content="Yufu Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Bungee+Shade&family=Titillium+Web&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>


<body>
<div class="main" style="max-width: 1080px;">
<div class="content">
<table>
<tbody>
<tr style="text-align:center">
    <span style="line-height:100%; font-size: 80%; font-weight: bold">aDiner: Adaptive Dynamic Implicit Neural Representation for Dynamic CBCT Imaging</td>
</tr>
<tr style="text-align:center">
    <td>
        <p style="font-size: 120%; line-height:100%;">
        <a href="../index.html">Yufu Zhou</a><sup style="color: #cb4b16; margin-right:0.4cm;">1</sup> 
        </p>
        <p style="font-size: 90%;"><sup style="margin-left:0cm; color: #cb4b16;">1</sup> Shanghai Jiao Tong University
        <sup style="margin-left:0.5cm; color: #859900;">2</sup> Shanghai Chest Hospital</p>
    </td>
</tr>
</tbody>
</table>
</div>

<div class="content">
<h2 style="text-align:center">Video</h2>
<table>
<tbody>
<div class="video-container">
<iframe width="560" height="315" src="simulated_regular.mp4" title="Simulated_regular_respiration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</tbody>
</table>
</div>

<div class="content">
<h2 style="text-align:center">Overview</h2>
<table>
<tbody>
<tr>
    <td>Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: <span style="color: #d4aa00;">static</span>, <span style="color: #2ca05c;">deforming</span>, and <span style="color: #0055d4;">new</span> areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and real-time rendering.</td>
</tr>
<!--
<tr style="text-align:center">
    <td><img src='images/nerfplayer-framework.png' width="100%"></td>
</tr>
-->
</tbody>
</table>
</div>

</div>
</body>
<script src="copy.js"></script>
<script src="script.js"></script>
